{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_interval=30, max_frames=100):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the video file.\n",
    "        frame_interval (int): Extract one frame every 'frame_interval' frames.\n",
    "        max_frames (int): Maximum number of frames to extract.\n",
    "\n",
    "    Returns:\n",
    "        List of extracted frames as BGR images.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Video file not found: {video_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    extracted = 0\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    expected_frames = min(max_frames, total_frames // frame_interval)\n",
    "    \n",
    "    with tqdm(total=expected_frames, desc=\"Extracting frames\") as pbar:\n",
    "        while cap.isOpened() and extracted < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if count % frame_interval == 0:\n",
    "                frames.append(frame)\n",
    "                extracted += 1\n",
    "                pbar.update(1)\n",
    "            count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Total extracted frames: {len(frames)}\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_compute_features(images):\n",
    "    \"\"\"\n",
    "    Detect SIFT features and compute descriptors for a list of images.\n",
    "\n",
    "    Parameters:\n",
    "        images (List of BGR images): List of images.\n",
    "\n",
    "    Returns:\n",
    "        keypoints_list: List of keypoints for each image.\n",
    "        descriptors_list: List of descriptors for each image.\n",
    "    \"\"\"\n",
    "    sift = cv2.SIFT_create() # nfeatures=5000)\n",
    "    keypoints_list = []\n",
    "    descriptors_list = []\n",
    "    num_keypoints = 0\n",
    "    pbar = tqdm(total=len(images), desc=\"Detecting features\", postfix={\"keypoints\": num_keypoints})\n",
    "\n",
    "    for idx, img in tqdm(enumerate(images), total=len(images), desc=\"Detecting features\", postfix={\"keypoints\": num_keypoints}):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        keypoints_list.append(keypoints)\n",
    "        descriptors_list.append(descriptors)\n",
    "        num_keypoints += len(keypoints)\n",
    "        pbar.set_postfix_str(f\"keypoints: {num_keypoints}\")\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return keypoints_list, descriptors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des1, des2, ratio=0.7):\n",
    "    \"\"\"\n",
    "    Match features between two sets of descriptors using FLANN and Lowe's ratio test.\n",
    "\n",
    "    Parameters:\n",
    "        des1: Descriptors from the first image.\n",
    "        des2: Descriptors from the second image.\n",
    "        ratio (float): Ratio threshold for Lowe's test.\n",
    "\n",
    "    Returns:\n",
    "        good_matches: List of good matches.\n",
    "    \"\"\"\n",
    "    if des1 is None or des2 is None:\n",
    "        return []\n",
    "    \n",
    "    # FLANN parameters for SIFT\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    try:\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "    except cv2.error as e:\n",
    "        print(\"Error during FLANN matching:\", e)\n",
    "        return []\n",
    "    \n",
    "    # Lowe's ratio test\n",
    "    good_matches = [m for m, n in matches if m.distance < ratio * n.distance]\n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(img1, kp1, img2, kp2, matches, max_matches=50):\n",
    "    \"\"\"\n",
    "    Visualize matches between two images.\n",
    "\n",
    "    Parameters:\n",
    "        img1: First image (BGR).\n",
    "        kp1: Keypoints from the first image.\n",
    "        img2: Second image (BGR).\n",
    "        kp2: Keypoints from the second image.\n",
    "        matches: List of matches.\n",
    "        max_matches: Maximum number of matches to display.\n",
    "    \"\"\"\n",
    "    matched_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:max_matches], None,\n",
    "                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(matched_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Top {max_matches} Matches\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|█████████▉| 2099/2100 [01:23<00:00, 24.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total extracted frames: 2099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Path to your video file\n",
    "video_path = 'dane6/rollei3.mov'  # Replace with your video path\n",
    "\n",
    "# Parameters\n",
    "frame_interval = 1   # Extract one frame every x frames\n",
    "max_frames = 4000    # Maximum number of frames to extract\n",
    "\n",
    "# Extract frames\n",
    "frames = extract_frames(video_path, frame_interval, max_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting features:  12%|█▏        | 245/2099 [02:23<17:18,  1.78it/s, keypoints: 4118701]"
     ]
    }
   ],
   "source": [
    "keypoints_list, descriptors_list = detect_and_compute_features(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have at least two frames\n",
    "q\n",
    "if len(frames) < 2:\n",
    "    print(\"Need at least two frames for reconstruction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Select the first two frames\n",
    "img1 = frames[100]\n",
    "img2 = frames[101]\n",
    "kp1  = keypoints_list[100]\n",
    "kp2  = keypoints_list[101]\n",
    "des1 = descriptors_list[100]\n",
    "des2 = descriptors_list[101] \n",
    "\n",
    "# Match features\n",
    "good_matches = match_features(des1, des2, ratio=0.8)\n",
    "print(f\"Number of good matches between Frame 1 and Frame 2: {len(good_matches)}\")\n",
    "\n",
    "# Optional: Visualize matches\n",
    "visualize_matches(img1, kp1, img2, kp2, good_matches, max_matches=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose_and_triangulate(kp1, kp2, matches, K, debug=False):\n",
    "    \"\"\"\n",
    "    Estimate camera pose and triangulate 3D points.\n",
    "    \"\"\"\n",
    "    if len(matches) < 8:\n",
    "        print(\"Not enough matches to compute Essential matrix.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    if debug:   \n",
    "        print(f\"Number of matches before RANSAC: {len(matches)}\")\n",
    "\n",
    "    # Compute Essential matrix with more relaxed threshold\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, \n",
    "                                  prob=0.999, threshold=3.0)  # Increased threshold\n",
    "    if debug:\n",
    "        print(f\"Number of matches after RANSAC: {len(matches)}\")\n",
    "    if E is None:\n",
    "        if debug:\n",
    "            print(\"Essential matrix estimation failed.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Recover pose\n",
    "    points_in_front, R, t, mask_pose = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Number of points in front of camera: {points_in_front}\")\n",
    "        print(\"Recovered Pose:\")\n",
    "        print(\"Rotation Matrix R:\")\n",
    "        print(R)\n",
    "        print(\"Translation Vector t:\")\n",
    "        print(t)\n",
    "\n",
    "    # Triangulate points\n",
    "    P1 = K @ np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "    P2 = K @ np.hstack((R, t))\n",
    "\n",
    "    # Convert points to homogeneous coordinates\n",
    "    pts1_h = cv2.convertPointsToHomogeneous(pts1)[:, 0, :]\n",
    "    pts2_h = cv2.convertPointsToHomogeneous(pts2)[:, 0, :]\n",
    "\n",
    "    # Triangulate all points\n",
    "    points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    points_3d = (points_4d / points_4d[3]).T[:, :3]\n",
    "\n",
    "    # Filter points based on positive depth and reprojection error\n",
    "    depths1 = points_3d[:, 2]\n",
    "    depths2 = (R @ points_3d.T + t).T[:, 2]\n",
    "    \n",
    "    # Keep points with positive depth in both views\n",
    "    mask_depths = (depths1 > 0) & (depths2 > 0)\n",
    "    points_3d = points_3d[mask_depths]\n",
    "    \n",
    "    # Center the points around origin\n",
    "    centroid = np.mean(points_3d, axis=0)\n",
    "    points_3d = points_3d - centroid\n",
    "    \n",
    "    # Scale the points to a reasonable size\n",
    "    scale = 10.0 / np.max(np.abs(points_3d))\n",
    "    points_3d = points_3d * scale\n",
    "    t = t * scale  # Scale translation vector accordingly\n",
    "    \n",
    "    if debug:\n",
    "        if len(points_3d) < 10:\n",
    "            print(f\"Too few valid 3D points: {len(points_3d)}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        print(f\"Number of valid 3D points: {len(points_3d)}\")\n",
    "    return R, t, points_3d, mask_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example intrinsic parameters based on FOV and image resolution\n",
    "f_x = 3225.6\n",
    "f_y = 3225.6\n",
    "c_x = img1.shape[1] / 2  # 4032 / 2 = 2016\n",
    "c_y = img1.shape[0] / 2  # 3024 / 2 = 1512\n",
    "\n",
    "K = np.array([[f_x, 0, c_x],\n",
    "              [0, f_y, c_y],\n",
    "              [0,  0,    1]])\n",
    "print(\"Updated Camera Intrinsic Matrix K:\")\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, t, points_3d, mask = estimate_pose_and_triangulate(kp1, kp2, good_matches, K)\n",
    "\n",
    "if R is None or t is None or points_3d is None:\n",
    "    print(\"Pose estimation or triangulation failed. Try adjusting parameters or using different frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_points(points_3d, percentile=95):\n",
    "    \"\"\"\n",
    "    Filter out outlier points based on distance from median.\n",
    "    \"\"\"\n",
    "    # Calculate distances from median point\n",
    "    median = np.median(points_3d, axis=0)\n",
    "    distances = np.linalg.norm(points_3d - median, axis=1)\n",
    "    \n",
    "    # Filter out points beyond the specified percentile\n",
    "    threshold = np.percentile(distances, percentile)\n",
    "    mask = distances < threshold\n",
    "    \n",
    "    return points_3d[mask]\n",
    "\n",
    "def visualize_3d_reconstruction(points_3d, R, t, K):\n",
    "    \"\"\"\n",
    "    Visualize 3D points and camera poses using Open3D with improved visualization.\n",
    "    \"\"\"\n",
    "    # Filter outlier points\n",
    "    filtered_points = filter_points(points_3d, percentile=98)\n",
    "    # filtered_points = points_3d\n",
    "    # Create Open3D point cloud\n",
    "    o3d_cloud = o3d.geometry.PointCloud()\n",
    "    o3d_cloud.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    \n",
    "    # Add colors based on height (z-coordinate)\n",
    "    colors = np.zeros((len(filtered_points), 3))\n",
    "    z_vals = filtered_points[:, 2]\n",
    "    z_min, z_max = np.min(z_vals), np.max(z_vals)\n",
    "    normalized_z = (z_vals - z_min) / (z_max - z_min)\n",
    "    \n",
    "    # Create a color gradient (blue to red)\n",
    "    colors[:, 0] = normalized_z  # Red channel\n",
    "    colors[:, 2] = 1 - normalized_z  # Blue channel\n",
    "    o3d_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Statistical outlier removal\n",
    "    # o3d_cloud, _ = o3d_cloud.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    # Create a visualization window\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=2560, height=1440)\n",
    "    \n",
    "    # Add geometries\n",
    "    vis.add_geometry(o3d_cloud)\n",
    "    \n",
    "    # Set rendering options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.point_size = 3.0  # Increased point size\n",
    "    opt.background_color = np.asarray([0.1, 0.1, 0.1])  # Dark gray background\n",
    "    \n",
    "    # Set camera viewpoint\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.7)\n",
    "    ctr.set_front([-0.5, -0.5, -0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, -1, 0])\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization from only 2 images\n",
    "# visualize_3d_reconstruction(points_3d, R, t, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_features_across_frames(frames, keypoints_list, descriptors_list):\n",
    "    \"\"\"\n",
    "    Track features across multiple frames and create feature tracks.\n",
    "    Returns a list of tracks, where each track is a dict containing frame indices and point locations.\n",
    "    \"\"\"\n",
    "    n_frames = len(frames)\n",
    "    tracks = []\n",
    "    next_track_id = 0\n",
    "    active_tracks = {}  # track_id -> (last_frame_idx, keypoint_idx, descriptor)\n",
    "    \n",
    "    for i in tqdm(range(n_frames - 1), desc=\"Tracking features\"):\n",
    "        # Match features with next frame\n",
    "        matches = match_features(descriptors_list[i], descriptors_list[i+1])\n",
    "        \n",
    "        # Update existing tracks and create new ones\n",
    "        current_tracks = {}\n",
    "        \n",
    "        for m in matches:\n",
    "            query_idx = m.queryIdx\n",
    "            train_idx = m.trainIdx\n",
    "            \n",
    "            # Check if this keypoint belongs to an existing track\n",
    "            track_found = False\n",
    "            for track_id, (frame_idx, kp_idx, _) in active_tracks.items():\n",
    "                if frame_idx == i and kp_idx == query_idx:\n",
    "                    # Extend existing track\n",
    "                    tracks[track_id]['frame_ids'].append(i + 1)\n",
    "                    tracks[track_id]['points'].append(keypoints_list[i+1][train_idx].pt)\n",
    "                    current_tracks[track_id] = (i + 1, train_idx, descriptors_list[i+1][train_idx])\n",
    "                    track_found = True\n",
    "                    break\n",
    "            \n",
    "            if not track_found:\n",
    "                # Create new track\n",
    "                track = {\n",
    "                    'track_id': next_track_id,\n",
    "                    'frame_ids': [i, i + 1],\n",
    "                    'points': [\n",
    "                        keypoints_list[i][query_idx].pt,\n",
    "                        keypoints_list[i+1][train_idx].pt\n",
    "                    ]\n",
    "                }\n",
    "                tracks.append(track)\n",
    "                current_tracks[next_track_id] = (i + 1, train_idx, descriptors_list[i+1][train_idx])\n",
    "                next_track_id += 1\n",
    "        \n",
    "        active_tracks = current_tracks\n",
    "    \n",
    "    # Filter tracks - keep only long tracks\n",
    "    min_track_length = 3  # Minimum number of frames a track should span\n",
    "    filtered_tracks = [t for t in tracks if len(t['frame_ids']) >= min_track_length]\n",
    "    \n",
    "    print(f\"Total tracks: {len(tracks)}\")\n",
    "    print(f\"Filtered tracks: {len(filtered_tracks)}\")\n",
    "    \n",
    "    return filtered_tracks\n",
    "\n",
    "def reconstruct_from_tracks(frames, K, tracks):\n",
    "    \"\"\"\n",
    "    Perform 3D reconstruction using feature tracks.\n",
    "    \"\"\"\n",
    "    # Initialize camera poses (first camera at origin)\n",
    "    n_frames = len(frames)\n",
    "    poses = [np.eye(4) for _ in range(n_frames)]  # Initialize all poses\n",
    "    \n",
    "    # Initialize 3D points\n",
    "    points_3d = []\n",
    "    point_colors = []\n",
    "    track_lengths = []\n",
    "    \n",
    "    # First, estimate relative poses between consecutive frames\n",
    "    for i in range(n_frames - 1):\n",
    "        # Find tracks that include both frames i and i+1\n",
    "        current_tracks = [t for t in tracks if i in t['frame_ids'] and i+1 in t['frame_ids']]\n",
    "        \n",
    "        if len(current_tracks) < 8:\n",
    "            continue\n",
    "            \n",
    "        # Get corresponding points\n",
    "        idx1 = [t['frame_ids'].index(i) for t in current_tracks]\n",
    "        idx2 = [t['frame_ids'].index(i+1) for t in current_tracks]\n",
    "        pts1 = np.float32([t['points'][i1] for t, i1 in zip(current_tracks, idx1)])\n",
    "        pts2 = np.float32([t['points'][i2] for t, i2 in zip(current_tracks, idx2)])\n",
    "        \n",
    "        # Estimate Essential matrix\n",
    "        E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "        if E is None:\n",
    "            continue\n",
    "            \n",
    "        # Recover relative pose\n",
    "        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "        \n",
    "        # Update pose (compose with previous pose)\n",
    "        rel_pose = np.eye(4)\n",
    "        rel_pose[:3, :3] = R\n",
    "        rel_pose[:3, 3] = t.flatten()\n",
    "        poses[i+1] = poses[i] @ rel_pose\n",
    "    \n",
    "    # Now triangulate points using all available views\n",
    "    for track in tqdm(tracks, desc=\"Triangulating points\"):\n",
    "        if len(track['frame_ids']) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Collect all observations for this track\n",
    "        frame_ids = track['frame_ids']\n",
    "        points_2d = np.float32([track['points'][i] for i in range(len(frame_ids))])\n",
    "        \n",
    "        # Normalize points\n",
    "        points_2d_norm = np.array([\n",
    "            cv2.undistortPoints(pt.reshape(1,1,2), K, None).reshape(2)\n",
    "            for pt in points_2d\n",
    "        ])\n",
    "        \n",
    "        # Triangulate using the first two views\n",
    "        P1 = poses[frame_ids[0]][:3]\n",
    "        P2 = poses[frame_ids[1]][:3]\n",
    "        \n",
    "        point_4d = cv2.triangulatePoints(P1, P2, \n",
    "                                       points_2d_norm[0].T,\n",
    "                                       points_2d_norm[1].T)\n",
    "        point_3d = (point_4d[:3] / point_4d[3]).reshape(3)\n",
    "        \n",
    "        # Verify reprojection error in all views\n",
    "        max_error = 0\n",
    "        for i, frame_id in enumerate(frame_ids):\n",
    "            P = poses[frame_id][:3]\n",
    "            projected = P @ np.append(point_3d, 1)\n",
    "            projected = projected[:2] / projected[2]\n",
    "            error = np.linalg.norm(points_2d_norm[i] - projected)\n",
    "            max_error = max(max_error, error)\n",
    "        \n",
    "        # Only keep points with small reprojection error\n",
    "        if max_error < 0.01:  # threshold in normalized coordinates\n",
    "            points_3d.append(point_3d)\n",
    "            # Get color from first frame\n",
    "            pt1 = points_2d[0].astype(int)\n",
    "            if 0 <= pt1[1] < frames[frame_ids[0]].shape[0] and 0 <= pt1[0] < frames[frame_ids[0]].shape[1]:\n",
    "                color = frames[frame_ids[0]][pt1[1], pt1[0]] / 255.0\n",
    "                point_colors.append(color)\n",
    "                track_lengths.append(len(frame_ids))\n",
    "    \n",
    "    if not points_3d:\n",
    "        return None, None, None\n",
    "        \n",
    "    points_3d = np.array(points_3d)\n",
    "    point_colors = np.array(point_colors)\n",
    "    \n",
    "    # Filter points based on track length\n",
    "    long_track_mask = np.array(track_lengths) >= 3\n",
    "    points_3d = points_3d[long_track_mask]\n",
    "    point_colors = point_colors[long_track_mask]\n",
    "    \n",
    "    # Remove outliers\n",
    "    filtered_points, filtered_colors = filter_points_and_colors(points_3d, point_colors, percentile=95)\n",
    "    \n",
    "    return filtered_points, filtered_colors, poses\n",
    "\n",
    "def filter_points_and_colors(points, colors, percentile=95):\n",
    "    \"\"\"\n",
    "    Filter outlier points using statistical analysis.\n",
    "    \"\"\"\n",
    "    # Calculate distances from median\n",
    "    median = np.median(points, axis=0)\n",
    "    distances = np.linalg.norm(points - median, axis=1)\n",
    "    \n",
    "    # Remove statistical outliers\n",
    "    mad = np.median(np.abs(distances - np.median(distances)))\n",
    "    modified_z_scores = 0.6745 * (distances - np.median(distances)) / mad\n",
    "    mask = modified_z_scores < 3.5  # Keep points within 3.5 MAD\n",
    "    \n",
    "    # Additional percentile-based filtering\n",
    "    threshold = np.percentile(distances, percentile)\n",
    "    mask &= distances < threshold\n",
    "    \n",
    "    return points[mask], colors[mask]\n",
    "\n",
    "def visualize_reconstruction(points, colors, poses):\n",
    "    \"\"\"\n",
    "    Visualize the complete reconstruction with colored points and camera poses.\n",
    "    \"\"\"\n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Create camera frustums\n",
    "    camera_frames = []\n",
    "    for pose in poses:\n",
    "        cam = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.5)\n",
    "        cam.transform(pose)\n",
    "        camera_frames.append(cam)\n",
    "    \n",
    "    # Visualization\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=1280, height=720)\n",
    "    \n",
    "    # Add geometries\n",
    "    vis.add_geometry(pcd)\n",
    "    for cam in camera_frames:\n",
    "        vis.add_geometry(cam)\n",
    "    \n",
    "    # Set rendering options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.point_size = 2.0\n",
    "    opt.background_color = np.asarray([0.1, 0.1, 0.1])\n",
    "    \n",
    "    # Set viewpoint\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.7)\n",
    "    ctr.set_front([-0.5, -0.5, -0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, -1, 0])\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track features across all frames\n",
    "tracks = track_features_across_frames(frames, keypoints_list, descriptors_list)\n",
    "\n",
    "# Perform reconstruction using tracks\n",
    "points_3d, colors, poses = reconstruct_from_tracks(frames, K, tracks)\n",
    "\n",
    "if points_3d is not None:\n",
    "    print(f\"Total reconstructed points: {len(points_3d)}\")\n",
    "    # Visualize the complete reconstruction\n",
    "    visualize_reconstruction(points_3d, colors, poses)\n",
    "else:\n",
    "    print(\"Reconstruction failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
