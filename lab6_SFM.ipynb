{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline SfM:\n",
    "\n",
    "- Detekcja cech\n",
    "  - SIFT\n",
    "  - SURF\n",
    "  - ORB\n",
    "- Dopasowanie cech między klatkami\n",
    "  - FLANN (cv2.FlannBasedMatcher)\n",
    "- Filtracja punktów\n",
    "- Estymacja położenia kamer\n",
    "- Triangulacja\n",
    "- visualize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_interval=30, max_frames=100):\n",
    "    \"\"\"\n",
    "    Extract frames from a video file.\n",
    "\n",
    "    Parameters:\n",
    "        video_path (str): Path to the video file.\n",
    "        frame_interval (int): Extract one frame every 'frame_interval' frames.\n",
    "        max_frames (int): Maximum number of frames to extract.\n",
    "\n",
    "    Returns:\n",
    "        List of extracted frames as BGR images.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(video_path):\n",
    "        print(f\"Video file not found: {video_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    count = 0\n",
    "    extracted = 0\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    expected_frames = min(max_frames, total_frames // frame_interval)\n",
    "    \n",
    "    with tqdm(total=expected_frames, desc=\"Extracting frames\") as pbar:\n",
    "        while cap.isOpened() and extracted < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if count % frame_interval == 0:\n",
    "                frames.append(frame)\n",
    "                extracted += 1\n",
    "                pbar.update(1)\n",
    "            count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Total extracted frames: {len(frames)}\")\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_compute_features(images):\n",
    "    \"\"\"\n",
    "    Detect SIFT features and compute descriptors with better parameters.\n",
    "    \"\"\"\n",
    "    # Create SIFT with more selective parameters\n",
    "    sift = cv2.SIFT_create(\n",
    "        nfeatures=2000,          # More features for better coverage\n",
    "        nOctaveLayers=3,         # Default value\n",
    "        contrastThreshold=0.04,  # Increased to get stronger features\n",
    "        edgeThreshold=10,        # Default value\n",
    "        sigma=1.6                # Default value\n",
    "    )\n",
    "    \n",
    "    keypoints_list = []\n",
    "    descriptors_list = []\n",
    "    num_keypoints = 0\n",
    "    \n",
    "    # Convert images to list if it's a generator or other iterable\n",
    "    images_list = list(images)\n",
    "    \n",
    "    for idx, img in tqdm(enumerate(images_list), total=len(images_list), desc=\"Detecting features\"):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Add Gaussian blur to reduce noise\n",
    "        gray = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        keypoints_list.append(keypoints)\n",
    "        descriptors_list.append(descriptors)\n",
    "        num_keypoints += len(keypoints)\n",
    "    \n",
    "    return keypoints_list, descriptors_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des1, des2, ratio=0.6):  # Stricter ratio test\n",
    "    \"\"\"\n",
    "    Match features with stricter filtering.\n",
    "    \"\"\"\n",
    "    if des1 is None or des2 is None:\n",
    "        return []\n",
    "    \n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=100)  # More checks for better accuracy\n",
    "    \n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    try:\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "    except cv2.error as e:\n",
    "        print(\"Error during FLANN matching:\", e)\n",
    "        return []\n",
    "    \n",
    "    # Stricter ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < ratio * n.distance and m.distance < 200:  # Added absolute distance threshold\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    return good_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(img1, kp1, img2, kp2, matches, max_matches=50):\n",
    "    \"\"\"\n",
    "    Visualize matches between two images.\n",
    "\n",
    "    Parameters:\n",
    "        img1: First image (BGR).\n",
    "        kp1: Keypoints from the first image.\n",
    "        img2: Second image (BGR).\n",
    "        kp2: Keypoints from the second image.\n",
    "        matches: List of matches.\n",
    "        max_matches: Maximum number of matches to display.\n",
    "    \"\"\"\n",
    "    matched_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:max_matches], None,\n",
    "                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(cv2.cvtColor(matched_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Top {max_matches} Matches\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_directory(directory_path, max_frames=100):\n",
    "    \"\"\"\n",
    "    Extract frames from a directory of images.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for file in os.listdir(directory_path):\n",
    "        if file.endswith(\".JPG\") or file.endswith(\".png\"):\n",
    "            img = cv2.imread(os.path.join(directory_path, file))\n",
    "            frames.append(img)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to your video file\n",
    "video_path = 'dane6/rollei3.mov'  # Replace with your video path\n",
    "\n",
    "# # Parameters\n",
    "frame_interval = 2   # Extract one frame every x frames\n",
    "max_frames = 30    # Maximum number of frames to extract\n",
    "\n",
    "# # Extract frames\n",
    "frames = extract_frames(video_path, frame_interval, max_frames)\n",
    "\n",
    "\n",
    "# directory_path = 'dane6/south-building/images'\n",
    "# # instead of video, get frames from directory\n",
    "# frames = extract_frames_from_directory(directory_path, max_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_list, descriptors_list = detect_and_compute_features(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have at least two frames\n",
    "if len(frames) < 2:\n",
    "    print(\"Need at least two frames for reconstruction.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Select the first two frames\n",
    "img1 = frames[0]\n",
    "img2 = frames[1]\n",
    "kp1  = keypoints_list[0]\n",
    "kp2  = keypoints_list[1]\n",
    "des1 = descriptors_list[0]\n",
    "des2 = descriptors_list[1] \n",
    "\n",
    "# Match features\n",
    "good_matches = match_features(des1, des2, ratio=0.8)\n",
    "print(f\"Number of good matches between Frame 1 and Frame 2: {len(good_matches)}\")\n",
    "\n",
    "# Optional: Visualize matches\n",
    "visualize_matches(img1, kp1, img2, kp2, good_matches, max_matches=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose_and_triangulate(kp1, kp2, matches, K, debug=False):\n",
    "    \"\"\"\n",
    "    Estimate camera pose and triangulate 3D points.\n",
    "    \"\"\"\n",
    "    if len(matches) < 8:\n",
    "        print(\"Not enough matches to compute Essential matrix.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    if debug:   \n",
    "        print(f\"Number of matches before RANSAC: {len(matches)}\")\n",
    "\n",
    "    # Compute Essential matrix with more relaxed threshold\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, \n",
    "                                  prob=0.999, threshold=3.0)\n",
    "    if debug:\n",
    "        print(f\"Number of matches after RANSAC: {len(matches)}\")\n",
    "    if E is None:\n",
    "        if debug:\n",
    "            print(\"Essential matrix estimation failed.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Recover pose\n",
    "    points_in_front, R, t, mask_pose = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Number of points in front of camera: {points_in_front}\")\n",
    "        print(\"Recovered Pose:\")\n",
    "        print(\"Rotation Matrix R:\")\n",
    "        print(R)\n",
    "        print(\"Translation Vector t:\")\n",
    "        print(t)\n",
    "\n",
    "    # Triangulate points\n",
    "    P1 = K @ np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "    P2 = K @ np.hstack((R, t))\n",
    "\n",
    "    # Convert points to homogeneous coordinates\n",
    "    pts1_h = cv2.convertPointsToHomogeneous(pts1)[:, 0, :]\n",
    "    pts2_h = cv2.convertPointsToHomogeneous(pts2)[:, 0, :]\n",
    "\n",
    "    # Triangulate all points\n",
    "    points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    points_3d = (points_4d / points_4d[3]).T[:, :3]\n",
    "\n",
    "    # Filter points based on positive depth and reprojection error\n",
    "    depths1 = points_3d[:, 2]\n",
    "    depths2 = (R @ points_3d.T + t).T[:, 2]\n",
    "    \n",
    "    # Keep points with positive depth in both views\n",
    "    mask_depths = (depths1 > 0) & (depths2 > 0)\n",
    "    points_3d = points_3d[mask_depths]\n",
    "    \n",
    "    # Center the points around origin\n",
    "    centroid = np.mean(points_3d, axis=0)\n",
    "    points_3d = points_3d - centroid\n",
    "    \n",
    "    # Scale the points to a reasonable size\n",
    "    scale = 10.0 / np.max(np.abs(points_3d))\n",
    "    points_3d = points_3d * scale\n",
    "    t = t * scale  # Scale translation vector accordingly\n",
    "    \n",
    "    if debug:\n",
    "        if len(points_3d) < 10:\n",
    "            print(f\"Too few valid 3D points: {len(points_3d)}\")\n",
    "            return None, None, None, None\n",
    "\n",
    "        print(f\"Number of valid 3D points: {len(points_3d)}\")\n",
    "    return R, t, points_3d, mask_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example intrinsic parameters based on FOV and image resolution\n",
    "f_x = 3225.6\n",
    "f_y = 3225.6\n",
    "c_x = img1.shape[1] / 2  # 4032 / 2 = 2016\n",
    "c_y = img1.shape[0] / 2  # 3024 / 2 = 1512\n",
    "\n",
    "K = np.array([[f_x, 0, c_x],\n",
    "              [0, f_y, c_y],\n",
    "              [0,  0,    1]])\n",
    "print(\"Updated Camera Intrinsic Matrix K:\")\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, t, points_3d, mask = estimate_pose_and_triangulate(kp1, kp2, good_matches, K)\n",
    "\n",
    "if R is None or t is None or points_3d is None:\n",
    "    print(\"Pose estimation or triangulation failed. Try adjusting parameters or using different frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_points(points_3d, percentile=95):\n",
    "    \"\"\"\n",
    "    Filter out outlier points based on distance from median.\n",
    "    \"\"\"\n",
    "    # Calculate distances from median point\n",
    "    median = np.median(points_3d, axis=0)\n",
    "    distances = np.linalg.norm(points_3d - median, axis=1)\n",
    "    \n",
    "    # Filter out points beyond the specified percentile\n",
    "    threshold = np.percentile(distances, percentile)\n",
    "    mask = distances < threshold\n",
    "    \n",
    "    return points_3d[mask]\n",
    "\n",
    "def visualize_3d_reconstruction(points_3d, R, t, K):\n",
    "    \"\"\"\n",
    "    Visualize 3D points and camera poses using Open3D with improved visualization.\n",
    "    \"\"\"\n",
    "    # Filter outlier points\n",
    "    filtered_points = filter_points(points_3d, percentile=98)\n",
    "    # filtered_points = points_3d\n",
    "    # Create Open3D point cloud\n",
    "    o3d_cloud = o3d.geometry.PointCloud()\n",
    "    o3d_cloud.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    \n",
    "    # Add colors based on height (z-coordinate)\n",
    "    colors = np.zeros((len(filtered_points), 3))\n",
    "    z_vals = filtered_points[:, 2]\n",
    "    z_min, z_max = np.min(z_vals), np.max(z_vals)\n",
    "    normalized_z = (z_vals - z_min) / (z_max - z_min)\n",
    "    \n",
    "    # Create a color gradient (blue to red)\n",
    "    colors[:, 0] = normalized_z  # Red channel\n",
    "    colors[:, 2] = 1 - normalized_z  # Blue channel\n",
    "    o3d_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Statistical outlier removal\n",
    "    # o3d_cloud, _ = o3d_cloud.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)\n",
    "    \n",
    "    # Create a visualization window\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=2560, height=1440)\n",
    "    \n",
    "    # Add geometries\n",
    "    vis.add_geometry(o3d_cloud)\n",
    "    \n",
    "    # Set rendering options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.point_size = 3.0  # Increased point size\n",
    "    opt.background_color = np.asarray([0.1, 0.1, 0.1])  # Dark gray background\n",
    "    \n",
    "    # Set camera viewpoint\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.7)\n",
    "    ctr.set_front([-0.5, -0.5, -0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, -1, 0])\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization from only 2 images\n",
    "visualize_3d_reconstruction(points_3d, R, t, K) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose_and_triangulate(kp1, kp2, matches, K, debug=False):\n",
    "    \"\"\"\n",
    "    Estimate camera pose and triangulate 3D points with improved error handling.\n",
    "    \"\"\"\n",
    "    if len(matches) < 8:\n",
    "        if debug:\n",
    "            print(\"Not enough matches to compute Essential matrix.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Extract matched keypoints\n",
    "    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "\n",
    "    # Compute Essential matrix\n",
    "    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, \n",
    "                                  prob=0.999, threshold=3.0)\n",
    "    \n",
    "    if E is None:\n",
    "        if debug:\n",
    "            print(\"Essential matrix estimation failed.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Recover pose\n",
    "    points_in_front, R, t, mask_pose = cv2.recoverPose(E, pts1, pts2, K)\n",
    "    \n",
    "    if points_in_front < 10:  # Add minimum points threshold\n",
    "        if debug:\n",
    "            print(f\"Too few points in front of camera: {points_in_front}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Triangulate points\n",
    "    P1 = K @ np.hstack((np.eye(3), np.zeros((3,1))))\n",
    "    P2 = K @ np.hstack((R, t))\n",
    "\n",
    "    pts1_h = cv2.convertPointsToHomogeneous(pts1)[:, 0, :]\n",
    "    pts2_h = cv2.convertPointsToHomogeneous(pts2)[:, 0, :]\n",
    "\n",
    "    points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)\n",
    "    points_3d = (points_4d / points_4d[3]).T[:, :3]\n",
    "\n",
    "    # Filter points based on positive depth\n",
    "    depths1 = points_3d[:, 2]\n",
    "    depths2 = (R @ points_3d.T + t).T[:, 2]\n",
    "    mask_depths = (depths1 > 0) & (depths2 > 0)\n",
    "    points_3d = points_3d[mask_depths]\n",
    "\n",
    "    if len(points_3d) < 10:  # Add minimum points threshold\n",
    "        if debug:\n",
    "            print(f\"Too few valid 3D points after filtering: {len(points_3d)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Center and scale points only if we have valid points\n",
    "    if len(points_3d) > 0:\n",
    "        centroid = np.mean(points_3d, axis=0)\n",
    "        points_3d = points_3d - centroid\n",
    "        \n",
    "        # Avoid division by zero in scaling\n",
    "        max_abs_val = np.max(np.abs(points_3d))\n",
    "        if max_abs_val > 1e-10:  # Add small threshold\n",
    "            scale = 10.0 / max_abs_val\n",
    "            points_3d = points_3d * scale\n",
    "            t = t * scale\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"Points too close to origin, skipping scaling\")\n",
    "    \n",
    "    return R, t, points_3d, mask_pose\n",
    "def reconstruct_sequence(frames, keypoints_list, descriptors_list, K, max_frame_gap=3):\n",
    "    \"\"\"\n",
    "    Reconstruct 3D scene with adaptive frame selection\n",
    "    \"\"\"\n",
    "    all_points_3d = []\n",
    "    all_cameras = []\n",
    "    \n",
    "    # Initialize first camera at origin\n",
    "    all_cameras.append((np.eye(3), np.zeros((3, 1))))\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(frames) - 1:\n",
    "        success = False\n",
    "        \n",
    "        # Try different frame gaps if initial matching fails\n",
    "        for frame_gap in range(1, max_frame_gap + 1):\n",
    "            if i + frame_gap >= len(frames):\n",
    "                break\n",
    "                \n",
    "            matches = match_features(descriptors_list[i], descriptors_list[i + frame_gap])\n",
    "            \n",
    "            if len(matches) >= 50:  # Require more matches for reliability\n",
    "                print(f\"Found {len(matches)} matches between frames {i} and {i + frame_gap}\")\n",
    "                \n",
    "                R, t, points_3d, mask = estimate_pose_and_triangulate(\n",
    "                    keypoints_list[i], \n",
    "                    keypoints_list[i + frame_gap], \n",
    "                    matches, \n",
    "                    K,\n",
    "                    debug=True\n",
    "                )\n",
    "                \n",
    "                if R is not None and t is not None and points_3d is not None and len(points_3d) > 20:\n",
    "                    # Transform points to global coordinate system\n",
    "                    if len(all_cameras) > 1:\n",
    "                        R_prev, t_prev = all_cameras[-1]\n",
    "                        R_new = R_prev @ R\n",
    "                        t_new = t_prev + R_prev @ t\n",
    "                        all_cameras.append((R_new, t_new))\n",
    "                        points_3d = (R_prev @ points_3d.T + t_prev).T\n",
    "                    else:\n",
    "                        all_cameras.append((R, t))\n",
    "                    \n",
    "                    all_points_3d.extend(points_3d)\n",
    "                    success = True\n",
    "                    i += frame_gap\n",
    "                    break\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"Failed to find good matches after frame {i}, skipping...\")\n",
    "            i += 1\n",
    "    \n",
    "    if len(all_points_3d) > 0:\n",
    "        all_points_3d = np.array(all_points_3d)\n",
    "        print(f\"Final reconstruction has {len(all_points_3d)} points and {len(all_cameras)} cameras\")\n",
    "    else:\n",
    "        print(\"Warning: No valid points reconstructed\")\n",
    "        return np.array([]), []\n",
    "    \n",
    "    return all_points_3d, all_cameras\n",
    "\n",
    "def visualize_full_reconstruction(all_points_3d, all_cameras):\n",
    "    \"\"\"\n",
    "    Visualize the complete 3D reconstruction with camera positions.\n",
    "    \"\"\"\n",
    "    # Filter outliers\n",
    "    filtered_points = filter_points(all_points_3d, percentile=98)\n",
    "    \n",
    "    # Create Open3D point cloud\n",
    "    o3d_cloud = o3d.geometry.PointCloud()\n",
    "    o3d_cloud.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "    \n",
    "    # Add colors based on height\n",
    "    colors = np.zeros((len(filtered_points), 3))\n",
    "    z_vals = filtered_points[:, 2]\n",
    "    z_min, z_max = np.min(z_vals), np.max(z_vals)\n",
    "    normalized_z = (z_vals - z_min) / (z_max - z_min)\n",
    "    colors[:, 0] = normalized_z\n",
    "    colors[:, 2] = 1 - normalized_z\n",
    "    o3d_cloud.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Create visualization\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=2560, height=1440)\n",
    "    vis.add_geometry(o3d_cloud)\n",
    "    \n",
    "    # Add camera frustums\n",
    "    for R, t in all_cameras:\n",
    "        # Create camera frustum geometry\n",
    "        cam_size = 0.1\n",
    "        camera_points = np.array([\n",
    "            [0, 0, 0],\n",
    "            [-cam_size, -cam_size, cam_size],\n",
    "            [cam_size, -cam_size, cam_size],\n",
    "            [cam_size, cam_size, cam_size],\n",
    "            [-cam_size, cam_size, cam_size]\n",
    "        ])\n",
    "        \n",
    "        # Transform camera points to global coordinate system\n",
    "        camera_points = (R @ camera_points.T + t).T\n",
    "        \n",
    "        # Create lines for camera frustum\n",
    "        lines = [[0, 1], [0, 2], [0, 3], [0, 4],\n",
    "                [1, 2], [2, 3], [3, 4], [4, 1]]\n",
    "        \n",
    "        # Create LineSet for camera frustum\n",
    "        line_set = o3d.geometry.LineSet()\n",
    "        line_set.points = o3d.utility.Vector3dVector(camera_points)\n",
    "        line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "        line_set.colors = o3d.utility.Vector3dVector([[1, 0, 0] for _ in range(len(lines))])\n",
    "        vis.add_geometry(line_set)\n",
    "    \n",
    "    # Set rendering options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.point_size = 3.0\n",
    "    opt.background_color = np.asarray([0.1, 0.1, 0.1])\n",
    "    \n",
    "    # Set camera viewpoint\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.7)\n",
    "    ctr.set_front([-0.5, -0.5, -0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, -1, 0])\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points_3d, all_cameras = reconstruct_sequence(frames, keypoints_list, descriptors_list, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_full_reconstruction(all_points_3d, all_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_features_across_frames(frames, keypoints_list, descriptors_list):\n",
    "    \"\"\"\n",
    "    Track features across multiple frames and create feature tracks.\n",
    "    Returns a list of tracks, where each track is a dict containing frame indices and point locations.\n",
    "    \"\"\"\n",
    "    n_frames = len(frames)\n",
    "    tracks = []\n",
    "    next_track_id = 0\n",
    "    active_tracks = {}  # track_id -> (last_frame_idx, keypoint_idx, descriptor)\n",
    "    \n",
    "    for i in tqdm(range(n_frames - 1), desc=\"Tracking features\"):\n",
    "        # Match features with next frame\n",
    "        matches = match_features(descriptors_list[i], descriptors_list[i+1])\n",
    "        \n",
    "        # Update existing tracks and create new ones\n",
    "        current_tracks = {}\n",
    "        \n",
    "        for m in matches:\n",
    "            query_idx = m.queryIdx\n",
    "            train_idx = m.trainIdx\n",
    "            \n",
    "            # Check if this keypoint belongs to an existing track\n",
    "            track_found = False\n",
    "            for track_id, (frame_idx, kp_idx, _) in active_tracks.items():\n",
    "                if frame_idx == i and kp_idx == query_idx:\n",
    "                    # Extend existing track\n",
    "                    tracks[track_id]['frame_ids'].append(i + 1)\n",
    "                    tracks[track_id]['points'].append(keypoints_list[i+1][train_idx].pt)\n",
    "                    current_tracks[track_id] = (i + 1, train_idx, descriptors_list[i+1][train_idx])\n",
    "                    track_found = True\n",
    "                    break\n",
    "            \n",
    "            if not track_found:\n",
    "                # Create new track\n",
    "                track = {\n",
    "                    'track_id': next_track_id,\n",
    "                    'frame_ids': [i, i + 1],\n",
    "                    'points': [\n",
    "                        keypoints_list[i][query_idx].pt,\n",
    "                        keypoints_list[i+1][train_idx].pt\n",
    "                    ]\n",
    "                }\n",
    "                tracks.append(track)\n",
    "                current_tracks[next_track_id] = (i + 1, train_idx, descriptors_list[i+1][train_idx])\n",
    "                next_track_id += 1\n",
    "        \n",
    "        active_tracks = current_tracks\n",
    "    \n",
    "    # Filter tracks - keep only long tracks\n",
    "    min_track_length = 3  # Minimum number of frames a track should span\n",
    "    filtered_tracks = [t for t in tracks if len(t['frame_ids']) >= min_track_length]\n",
    "    \n",
    "    print(f\"Total tracks: {len(tracks)}\")\n",
    "    print(f\"Filtered tracks: {len(filtered_tracks)}\")\n",
    "    \n",
    "    return filtered_tracks\n",
    "\n",
    "def reconstruct_from_tracks(frames, K, tracks):\n",
    "    \"\"\"\n",
    "    Perform 3D reconstruction using feature tracks.\n",
    "    \"\"\"\n",
    "    # Initialize camera poses (first camera at origin)\n",
    "    n_frames = len(frames)\n",
    "    poses = [np.eye(4) for _ in range(n_frames)]  # Initialize all poses\n",
    "    \n",
    "    # Initialize 3D points\n",
    "    points_3d = []\n",
    "    point_colors = []\n",
    "    track_lengths = []\n",
    "    \n",
    "    # First, estimate relative poses between consecutive frames\n",
    "    for i in range(n_frames - 1):\n",
    "        # Find tracks that include both frames i and i+1\n",
    "        current_tracks = [t for t in tracks if i in t['frame_ids'] and i+1 in t['frame_ids']]\n",
    "        \n",
    "        if len(current_tracks) < 8:\n",
    "            continue\n",
    "            \n",
    "        # Get corresponding points\n",
    "        idx1 = [t['frame_ids'].index(i) for t in current_tracks]\n",
    "        idx2 = [t['frame_ids'].index(i+1) for t in current_tracks]\n",
    "        pts1 = np.float32([t['points'][i1] for t, i1 in zip(current_tracks, idx1)])\n",
    "        pts2 = np.float32([t['points'][i2] for t, i2 in zip(current_tracks, idx2)])\n",
    "        \n",
    "        # Estimate Essential matrix\n",
    "        E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "        if E is None:\n",
    "            continue\n",
    "            \n",
    "        # Recover relative pose\n",
    "        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n",
    "        \n",
    "        # Update pose (compose with previous pose)\n",
    "        rel_pose = np.eye(4)\n",
    "        rel_pose[:3, :3] = R\n",
    "        rel_pose[:3, 3] = t.flatten()\n",
    "        poses[i+1] = poses[i] @ rel_pose\n",
    "    \n",
    "    # Now triangulate points using all available views\n",
    "    for track in tqdm(tracks, desc=\"Triangulating points\"):\n",
    "        if len(track['frame_ids']) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Collect all observations for this track\n",
    "        frame_ids = track['frame_ids']\n",
    "        points_2d = np.float32([track['points'][i] for i in range(len(frame_ids))])\n",
    "        \n",
    "        # Normalize points\n",
    "        points_2d_norm = np.array([\n",
    "            cv2.undistortPoints(pt.reshape(1,1,2), K, None).reshape(2)\n",
    "            for pt in points_2d\n",
    "        ])\n",
    "        \n",
    "        # Triangulate using the first two views\n",
    "        P1 = poses[frame_ids[0]][:3]\n",
    "        P2 = poses[frame_ids[1]][:3]\n",
    "        \n",
    "        point_4d = cv2.triangulatePoints(P1, P2, \n",
    "                                       points_2d_norm[0].T,\n",
    "                                       points_2d_norm[1].T)\n",
    "        point_3d = (point_4d[:3] / point_4d[3]).reshape(3)\n",
    "        \n",
    "        # Verify reprojection error in all views\n",
    "        max_error = 0\n",
    "        for i, frame_id in enumerate(frame_ids):\n",
    "            P = poses[frame_id][:3]\n",
    "            projected = P @ np.append(point_3d, 1)\n",
    "            projected = projected[:2] / projected[2]\n",
    "            error = np.linalg.norm(points_2d_norm[i] - projected)\n",
    "            max_error = max(max_error, error)\n",
    "        \n",
    "        # Only keep points with small reprojection error\n",
    "        if max_error < 0.01:  # threshold in normalized coordinates\n",
    "            points_3d.append(point_3d)\n",
    "            # Get color from first frame\n",
    "            pt1 = points_2d[0].astype(int)\n",
    "            if 0 <= pt1[1] < frames[frame_ids[0]].shape[0] and 0 <= pt1[0] < frames[frame_ids[0]].shape[1]:\n",
    "                color = frames[frame_ids[0]][pt1[1], pt1[0]] / 255.0\n",
    "                point_colors.append(color)\n",
    "                track_lengths.append(len(frame_ids))\n",
    "    \n",
    "    if not points_3d:\n",
    "        return None, None, None\n",
    "        \n",
    "    points_3d = np.array(points_3d)\n",
    "    point_colors = np.array(point_colors)\n",
    "    \n",
    "    # Filter points based on track length\n",
    "    long_track_mask = np.array(track_lengths) >= 3\n",
    "    points_3d = points_3d[long_track_mask]\n",
    "    point_colors = point_colors[long_track_mask]\n",
    "    \n",
    "    # Remove outliers\n",
    "    filtered_points, filtered_colors = filter_points_and_colors(points_3d, point_colors, percentile=95)\n",
    "    \n",
    "    return filtered_points, filtered_colors, poses\n",
    "\n",
    "def filter_points_and_colors(points, colors, percentile=98):\n",
    "    \"\"\"\n",
    "    More aggressive point cloud filtering.\n",
    "    \"\"\"\n",
    "    # Remove statistical outliers\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Statistical outlier removal\n",
    "    pcd, ind = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=1.0)\n",
    "    \n",
    "    # Voxel downsampling to reduce density\n",
    "    voxel_size = np.mean(pcd.get_axis_aligned_bounding_box().get_extent()) * 0.005\n",
    "    pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "    \n",
    "    # Convert back to numpy arrays\n",
    "    points_filtered = np.asarray(pcd.points)\n",
    "    colors_filtered = np.asarray(pcd.colors)\n",
    "    \n",
    "    return points_filtered, colors_filtered\n",
    "\n",
    "def visualize_reconstruction(points, colors, poses):\n",
    "    \"\"\"\n",
    "    Visualize the complete reconstruction with colored points and camera poses.\n",
    "    \"\"\"\n",
    "    # Create point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    \n",
    "    # Create camera frustums\n",
    "    camera_frames = []\n",
    "    for pose in poses:\n",
    "        cam = o3d.geometry.TriangleMesh.create_coordinate_frame(size=0.2)\n",
    "        cam.transform(pose)\n",
    "        camera_frames.append(cam)\n",
    "    \n",
    "    # Visualization\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(width=2560, height=1440)\n",
    "    \n",
    "    # Add geometries\n",
    "    vis.add_geometry(pcd)\n",
    "    for cam in camera_frames:\n",
    "        vis.add_geometry(cam)\n",
    "    # Set rendering options\n",
    "    opt = vis.get_render_option()\n",
    "    opt.point_size = 2.0\n",
    "    opt.background_color = np.asarray([0.1, 0.1, 0.1])\n",
    "    \n",
    "    # Set viewpoint\n",
    "    ctr = vis.get_view_control()\n",
    "    ctr.set_zoom(0.7)\n",
    "    ctr.set_front([-0.5, -0.5, -0.5])\n",
    "    ctr.set_lookat([0, 0, 0])\n",
    "    ctr.set_up([0, -1, 0])\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track features across all frames \n",
    "tracks = track_features_across_frames(frames, keypoints_list, descriptors_list)\n",
    "\n",
    "# Perform reconstruction using tracks\n",
    "points_3d, colors, poses = reconstruct_from_tracks(frames, K, tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(points_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_reconstruction(points_3d, colors, poses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
